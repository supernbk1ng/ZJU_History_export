import json
import os
import time
import re
from typing import List, Dict

class ZJUHistoryDataCollector:
    def __init__(self):
        self.data_dir = "raw_data/documents"
        
    def load_local_documents(self) -> List[Dict]:
        """åŠ è½½æœ¬åœ°æ–‡æ¡£èµ„æ–™"""
        documents = []
        
        # å®šä¹‰è¦å¤„ç†çš„æ–‡æ¡£
        local_files = {
            "zju_history.txt": "æ ¡å²æ¦‚è¿°",
            "zju_history_baidu.txt": "ç™¾åº¦ç™¾ç§‘èµ„æ–™", 
            "zju_history_wiki.txt": "ç»´åŸºç™¾ç§‘èµ„æ–™"
        }
        
        for filename, doc_type in local_files.items():
            filepath = os.path.join(self.data_dir, filename)
            try:
                if os.path.exists(filepath):
                    print(f"ğŸ“– æ­£åœ¨è¯»å–: {filename}")
                    with open(filepath, "r", encoding="utf-8") as f:
                        content = f.read()
                    
                    if content.strip():
                        documents.append({
                            "type": doc_type,
                            "filename": filename,
                            "content": content,
                            "source": "æœ¬åœ°æ–‡æ¡£",
                            "collected_time": time.strftime("%Y-%m-%d %H:%M:%S")
                        })
                        print(f"âœ… æˆåŠŸè¯»å–: {filename} ({len(content)} å­—ç¬¦)")
                    else:
                        print(f"âš ï¸ æ–‡ä»¶ä¸ºç©º: {filename}")
                else:
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                    
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        print(f"ğŸ“Š æ€»å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents

class DataEnhancer:
    def __init__(self):
        self.enhanced_data = []
    
    def enhance_existing_data(self, raw_data: List[Dict]):
        """å¢å¼ºç°æœ‰æ•°æ®"""
        print("ğŸ”§ å¼€å§‹æ•°æ®å¢å¼ºå¤„ç†...")
        
        for item in raw_data:
            enhanced_item = self._enhance_single_item(item)
            self.enhanced_data.append(enhanced_item)
        
        print(f"âœ… æ•°æ®å¢å¼ºå®Œæˆï¼Œå…±å¤„ç† {len(self.enhanced_data)} æ¡æ•°æ®")
        return self.enhanced_data
    
    def _enhance_single_item(self, item: Dict) -> Dict:
        """å¢å¼ºå•ä¸ªæ•°æ®é¡¹"""
        content = item.get('content', '')
        
        # æ•°æ®æ¸…æ´—
        cleaned_content = self._clean_content(content)
        
        # ç»“æ„åŒ–å¤„ç†
        structured_data = self._structure_content(cleaned_content)
        
        # æ·»åŠ å¢å¼ºä¿¡æ¯
        enhanced_item = {
            **item,
            "cleaned_content": cleaned_content,
            "structured_data": structured_data,
            "enhancement_level": self._assess_enhancement_level(cleaned_content),
            "word_count": len(cleaned_content),
            "key_topics": self._extract_key_topics(cleaned_content),
            "enhanced_time": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return enhanced_item
    
    def _clean_content(self, content: str) -> str:
        """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡æ ‡ç‚¹
        cleaned = re.sub(r'[^\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹\s\w]', '', content)
        
        # åˆå¹¶å¤šä¸ªç©ºç™½å­—ç¬¦
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # ç§»é™¤é¦–å°¾ç©ºç™½
        cleaned = cleaned.strip()
        
        return cleaned
    
    def _structure_content(self, content: str) -> Dict:
        """å°†å†…å®¹ç»“æ„åŒ–"""
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # è¯†åˆ«æ®µè½
        paragraphs = []
        current_para = []
        
        for sentence in sentences:
            current_para.append(sentence)
            # å¦‚æœå¥å­åŒ…å«æ—¶é—´ä¿¡æ¯æˆ–è¾¾åˆ°ä¸€å®šé•¿åº¦ï¼Œå¼€å§‹æ–°æ®µè½
            if re.search(r'\d{4}å¹´', sentence) or len(current_para) >= 3:
                if current_para:
                    paragraphs.append('ã€‚'.join(current_para) + 'ã€‚')
                    current_para = []
        
        if current_para:
            paragraphs.append('ã€‚'.join(current_para) + 'ã€‚')
        
        return {
            "sentences": sentences,
            "paragraphs": paragraphs,
            "time_periods": re.findall(r'\d{4}å¹´', content),
            "key_figures": self._extract_figures(content),
            "locations": self._extract_locations(content)
        }
    
    def _extract_figures(self, content: str) -> List[str]:
        """æå–äººç‰©å§“å"""
        zju_figures = ["ç«ºå¯æ¡¢", "æ—å¯", "è’‹æ¢¦éºŸ", "é™ˆå»ºåŠŸ", "è‹æ­¥é’", "æŸæ˜ŸåŒ—", 
                      "è´æ—¶ç’‹", "è”¡é‚¦å", "é©¬ä¸€æµ®", "ä¸°å­æº", "é’±ç©†", "ç‹æ·¦æ˜Œ",
                      "è°ˆå®¶æ¡¢", "ææ”¿é“", "ç¨‹å¼€ç”²", "è°·è¶…è±ª", "å¶ç¬ƒæ­£", "æçº¦ç‘Ÿ",
                      "é‚µé£˜è", "ä½•ç‡æ—¶", "è’‹æ–¹éœ‡", "è´¹å·©", "äºå­ä¸‰", "é©¬å¯…åˆ"]
        
        found_figures = []
        for figure in zju_figures:
            if figure in content:
                found_figures.append(figure)
        
        return found_figures
    
    def _extract_locations(self, content: str) -> List[str]:
        """æå–åœ°ç‚¹"""
        zju_locations = ["æ­å·", "å»ºå¾·", "å‰å®‰", "æ³°å’Œ", "å®œå±±", "éµä¹‰", "æ¹„æ½­",
                        "å¤©ç›®å±±", "ç¦…æºå¯º", "ç´«é‡‘æ¸¯", "ç‰æ³‰", "ä¹‹æ±Ÿ", "åå®¶æ± ",
                        "é¾™æ³‰", "æ¾æœ¨åœº", "æ¹–æ»¨", "èˆŸå±±", "æµ·å®", "å®æ³¢"]
        
        found_locations = []
        for location in zju_locations:
            if location in content:
                found_locations.append(location)
        
        return found_locations
    
    def _extract_key_topics(self, content: str) -> List[str]:
        """æå–å…³é”®ä¸»é¢˜"""
        topics = []
        
        key_themes = {
            "è¥¿è¿": ["è¥¿è¿", "è¿æ ¡", "æ¬è¿", "é•¿å¾", "æ–‡å†›é•¿å¾"],
            "åˆå¹¶": ["åˆå¹¶", "å››æ ¡åˆå¹¶", "é‡ç»„", "ç»„å»º"],
            "åˆ›ç«‹": ["åˆ›ç«‹", "åˆ›å»º", "æˆç«‹", "åˆ›åŠ", "å»ºç«‹"],
            "å‘å±•": ["å‘å±•", "å»ºè®¾", "æ‰©å»º", "å£®å¤§", "è°ƒæ•´"],
            "æˆå°±": ["æˆå°±", "æˆæœ", "è´¡çŒ®", "è·å¥–", "å“è¶Š"],
            "æ”¹é©": ["æ”¹é©", "æ”¹åˆ¶", "è°ƒæ•´", "æ”¹é€ "]
        }
        
        for theme, keywords in key_themes.items():
            if any(keyword in content for keyword in keywords):
                topics.append(theme)
        
        return topics
    
    def _assess_enhancement_level(self, content: str) -> str:
        """è¯„ä¼°æ•°æ®è´¨é‡ç­‰çº§"""
        word_count = len(content)
        time_refs = len(re.findall(r'\d{4}å¹´', content))
        figure_refs = len(self._extract_figures(content))
        
        if word_count > 800 and (time_refs > 3 or figure_refs > 2):
            return "é«˜è´¨é‡"
        elif word_count > 300:
            return "ä¸­ç­‰è´¨é‡"
        else:
            return "åŸºç¡€è´¨é‡"

class AdvancedChunker:
    def __init__(self):
        self.chunk_size = 400
        self.overlap = 50
    
    def chunk_enhanced_data(self, enhanced_data: List[Dict]) -> List[Dict]:
        """å¯¹å¢å¼ºæ•°æ®è¿›è¡Œæ™ºèƒ½åˆ†å—"""
        chunks = []
        
        for item in enhanced_data:
            structured = item.get('structured_data', {})
            paragraphs = structured.get('paragraphs', [])
            
            for i, paragraph in enumerate(paragraphs):
                # å¦‚æœæ®µè½å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
                if len(paragraph) > self.chunk_size:
                    sub_chunks = self._split_long_paragraph(paragraph)
                    for j, sub_chunk in enumerate(sub_chunks):
                        chunk = self._create_chunk(item, sub_chunk, i, j)
                        chunks.append(chunk)
                else:
                    chunk = self._create_chunk(item, paragraph, i)
                    chunks.append(chunk)
        
        print(f"âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks
    
    def _split_long_paragraph(self, paragraph: str) -> List[str]:
        """åˆ†å‰²é•¿æ®µè½"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', paragraph)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                if current_chunk:
                    current_chunk += "ã€‚" + sentence
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk + "ã€‚")
                    current_chunk = sentence
                else:
                    # å•ä¸ªå¥å­å°±è¶…è¿‡chunk_sizeï¼Œå¼ºåˆ¶åˆ†å‰²
                    chunks.append(sentence + "ã€‚")
                    current_chunk = ""
        
        if current_chunk:
            chunks.append(current_chunk + "ã€‚")
        
        return chunks
    
    def _create_chunk(self, item: Dict, content: str, para_index: int, sub_index: int = None) -> Dict:
        """åˆ›å»ºæ ‡å‡†åŒ–çš„æ•°æ®å—"""
        chunk_id = f"{item.get('filename', 'doc')}_p{para_index}"
        if sub_index is not None:
            chunk_id += f"_s{sub_index}"
        
        return {
            "id": chunk_id,
            "content": content,
            "source": item.get('source', 'æœ¬åœ°æ–‡æ¡£'),
            "filename": item.get('filename', 'unknown'),
            "original_type": item.get('type', 'unknown'),
            "enhancement_level": item.get('enhancement_level', 'unknown'),
            "key_topics": item.get('key_topics', []),
            "word_count": len(content),
            "time_periods": re.findall(r'\d{4}å¹´', content),
            "figures": self._extract_figures_from_chunk(content),
            "locations": self._extract_locations_from_chunk(content),
            "chunk_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def _extract_figures_from_chunk(self, content: str) -> List[str]:
        """ä»å—ä¸­æå–äººç‰©"""
        figures = ["ç«ºå¯æ¡¢", "æ—å¯", "è’‹æ¢¦éºŸ", "é™ˆå»ºåŠŸ", "è‹æ­¥é’", "æŸæ˜ŸåŒ—", 
                  "è´æ—¶ç’‹", "è”¡é‚¦å", "é©¬ä¸€æµ®", "ä¸°å­æº", "é’±ç©†", "ç‹æ·¦æ˜Œ",
                  "è°ˆå®¶æ¡¢", "ææ”¿é“", "ç¨‹å¼€ç”²", "è°·è¶…è±ª", "å¶ç¬ƒæ­£", "æçº¦ç‘Ÿ"]
        
        return [f for f in figures if f in content]
    
    def _extract_locations_from_chunk(self, content: str) -> List[str]:
        """ä»å—ä¸­æå–åœ°ç‚¹"""
        locations = ["æ­å·", "å»ºå¾·", "å‰å®‰", "æ³°å’Œ", "å®œå±±", "éµä¹‰", "æ¹„æ½­",
                    "å¤©ç›®å±±", "ç¦…æºå¯º", "ç´«é‡‘æ¸¯", "ç‰æ³‰", "ä¹‹æ±Ÿ", "åå®¶æ± "]
        
        return [l for l in locations if l in content]

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ•°æ®å¢å¼ºæµç¨‹"""
    print("ğŸš€ å¼€å§‹æµ™å¤§æ ¡å²æ•°æ®å¢å¼ºæµç¨‹...")
    
    # 1. æ•°æ®æ”¶é›†
    collector = ZJUHistoryDataCollector()
    print("ğŸ“¥ é˜¶æ®µ1: åŠ è½½æœ¬åœ°æ–‡æ¡£")
    local_data = collector.load_local_documents()
    
    if not local_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ raw_data/documents/ ç›®å½•")
        return
    
    # 2. æ•°æ®å¢å¼º
    print("ğŸ”§ é˜¶æ®µ2: æ•°æ®å¢å¼º")
    enhancer = DataEnhancer()
    enhanced_data = enhancer.enhance_existing_data(local_data)
    
    # ä¿å­˜å¢å¼ºæ•°æ®
    os.makedirs("processed_data", exist_ok=True)
    with open("processed_data/enhanced_raw_data.json", "w", encoding="utf-8") as f:
        json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
    
    # 3. æ™ºèƒ½åˆ†å—
    print("âœ‚ï¸ é˜¶æ®µ3: æ™ºèƒ½åˆ†å—")
    chunker = AdvancedChunker()
    chunks = chunker.chunk_enhanced_data(enhanced_data)
    
    # ä¿å­˜åˆ†å—æ•°æ®
    with open("processed_data/enhanced_chunks.json", "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    
    # 4. ç»Ÿè®¡ä¿¡æ¯
    total_words = sum(chunk['word_count'] for chunk in chunks)
    total_figures = sum(len(chunk['figures']) for chunk in chunks)
    total_locations = sum(len(chunk['locations']) for chunk in chunks)
    avg_chunk_size = total_words / len(chunks) if chunks else 0
    
    print(f"""
ğŸ‰ æ•°æ®å¢å¼ºå®Œæˆï¼

ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:
â”œâ”€â”€ åŸå§‹æ–‡æ¡£: {len(local_data)} ä¸ª
â”œâ”€â”€ ç”Ÿæˆæ–‡æœ¬å—: {len(chunks)} ä¸ª
â”œâ”€â”€ æ€»å­—æ•°: {total_words} å­—
â”œâ”€â”€ æ¶‰åŠäººç‰©: {total_figures} æ¬¡
â”œâ”€â”€ æ¶‰åŠåœ°ç‚¹: {total_locations} æ¬¡
â”œâ”€â”€ å¹³å‡å—å¤§å°: {avg_chunk_size:.1f} å­—
â””â”€â”€ é«˜è´¨é‡å—: {sum(1 for c in chunks if c.get('enhancement_level') == 'é«˜è´¨é‡')} ä¸ª

ğŸ’¾ è¾“å‡ºæ–‡ä»¶:
â”œâ”€â”€ processed_data/enhanced_raw_data.json (å¢å¼ºçš„åŸå§‹æ•°æ®)
â””â”€â”€ processed_data/enhanced_chunks.json (æ™ºèƒ½åˆ†å—æ•°æ®)

æ¥ä¸‹æ¥è¯·è¿è¡Œ: python rebuild_vector_db.py
    """)

if __name__ == "__main__":
    main()