import re
import json
import os
from typing import List, Dict, Tuple
from datetime import datetime

class ZJUDocumentCleaner:
    def __init__(self):
        self.cleaned_documents = []
        
    def clean_all_documents(self):
        """æ¸…æ´—æ‰€æœ‰æ–‡æ¡£"""
        documents_info = [
            ("zju_history.txt", "æ ¡å²æ¦‚è¿°"),
            ("zju_history_baidu.txt", "ç™¾åº¦ç™¾ç§‘"), 
            ("zju_history_wiki.txt", "ç»´åŸºç™¾ç§‘")
        ]
        
        for filename, source in documents_info:
            print(f"ğŸ§¹ æ­£åœ¨æ¸…æ´—: {filename}")
            cleaned_content = self.clean_single_document(filename, source)
            if cleaned_content:
                self.cleaned_documents.append(cleaned_content)
        
        # ä¿å­˜æ¸…æ´—åçš„æ–‡æ¡£
        self.save_cleaned_documents()
        return self.cleaned_documents
    
    def clean_single_document(self, filename: str, source: str) -> Dict:
        """æ¸…æ´—å•ä¸ªæ–‡æ¡£"""
        filepath = f"raw_data/documents/{filename}"
        
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
            
            print(f"  åŸå§‹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # åˆ†æ­¥éª¤æ¸…æ´—
            content = self.remove_reference_marks(content)  # ç§»é™¤å¼•ç”¨æ ‡è®°
            content = self.clean_formatting(content)        # æ¸…ç†æ ¼å¼
            content = self.normalize_dates(content)         # æ ‡å‡†åŒ–æ—¥æœŸ
            content = self.split_long_paragraphs(content)   # åˆ†å‰²é•¿æ®µè½
            content = self.remove_redundant_info(content)   # ç§»é™¤å†—ä½™ä¿¡æ¯
            
            # ç»“æ„åŒ–å¤„ç†
            structured_content = self.structure_content(content, filename)
            
            print(f"  æ¸…æ´—å: {len(content)} å­—ç¬¦")
            
            return {
                "filename": filename,
                "source": source,
                "original_length": len(content),
                "cleaned_length": len(structured_content.get('content', '')),
                "content": structured_content.get('content', ''),
                "paragraphs": structured_content.get('paragraphs', []),
                "time_periods": structured_content.get('time_periods', []),
                "key_figures": structured_content.get('key_figures', []),
                "key_locations": structured_content.get('key_locations', []),
                "cleaned_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            print(f"âŒ æ¸…æ´—æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            return None
    
    def remove_reference_marks(self, content: str) -> str:
        """ç§»é™¤å¼•ç”¨æ ‡è®°å’Œæ³¨é‡Š"""
        # ç§»é™¤ [æ•°å­—] æ ¼å¼çš„å¼•ç”¨æ ‡è®°
        content = re.sub(r'\[\d+\]', '', content)
        # ç§»é™¤ [éœ€è¦è§£é‡Š] ç­‰æ³¨é‡Š
        content = re.sub(r'\[[^\]]*?\]', '', content)
        # ç§»é™¤ (ä¸»è¯æ¡ï¼š...) ç­‰è¯´æ˜
        content = re.sub(r'ï¼ˆä¸»è¯æ¡ï¼š[^ï¼‰]*?ï¼‰', '', content)
        return content
    
    def clean_formatting(self, content: str) -> str:
        """æ¸…ç†æ ¼å¼"""
        # ç»Ÿä¸€æ¢è¡Œç¬¦
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œç©ºç™½å­—ç¬¦
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        
        # é‡æ–°ç»„åˆï¼Œç¡®ä¿æ®µè½ä¹‹é—´æœ‰é€‚å½“çš„é—´è·
        cleaned_lines = []
        for i, line in enumerate(lines):
            # å¦‚æœå½“å‰è¡Œæ˜¯æ ‡é¢˜æ ¼å¼ï¼ˆçŸ­æ–‡æœ¬ä¸”æ²¡æœ‰å¥å·ï¼‰ï¼Œå•ç‹¬æˆæ®µ
            if len(line) < 50 and 'ã€‚' not in line and i < len(lines)-1:
                cleaned_lines.append(line)
                cleaned_lines.append('')  # ç©ºè¡Œåˆ†éš”
            else:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def normalize_dates(self, content: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        # ç»Ÿä¸€å¹´ä»½è¡¨ç¤º
        content = re.sub(r'(\d{4})â€”(\d{4})', r'\1-\2', content)  # æ›¿æ¢å…¨è§’ç ´æŠ˜å·
        content = re.sub(r'(\d{4})-(\d{4})', r'\1è‡³\2', content)  # ç»Ÿä¸€ä¸º"è‡³"
        
        # æ ‡å‡†åŒ–æ—¶é—´èŒƒå›´
        content = re.sub(r'ï¼ˆ(\d{4})-(\d{4})ï¼‰', r'ï¼ˆ\1è‡³\2ï¼‰', content)
        
        return content
    
    def split_long_paragraphs(self, content: str) -> str:
        """åˆ†å‰²è¿‡é•¿çš„æ®µè½"""
        paragraphs = content.split('\n\n')
        result_paragraphs = []
        
        for para in paragraphs:
            if len(para) > 500:  # å¦‚æœæ®µè½è¶…è¿‡500å­—ï¼Œè¿›è¡Œåˆ†å‰²
                # æŒ‰å¥å­åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', para)
                sentences = [s.strip() for s in sentences if s.strip()]
                
                current_chunk = []
                current_length = 0
                
                for sentence in sentences:
                    if current_length + len(sentence) > 300 and current_chunk:
                        # ä¿å­˜å½“å‰å—
                        result_paragraphs.append('ã€‚'.join(current_chunk) + 'ã€‚')
                        current_chunk = [sentence]
                        current_length = len(sentence)
                    else:
                        current_chunk.append(sentence)
                        current_length += len(sentence)
                
                if current_chunk:
                    result_paragraphs.append('ã€‚'.join(current_chunk) + 'ã€‚')
            else:
                result_paragraphs.append(para)
        
        return '\n\n'.join(result_paragraphs)
    
    def remove_redundant_info(self, content: str) -> str:
        """ç§»é™¤å†—ä½™ä¿¡æ¯"""
        # ç§»é™¤è¿‡äºè¯¦ç»†çš„é™¢ç³»è°ƒæ•´åˆ—è¡¨ï¼ˆä¿ç•™æ¦‚æ‹¬æ€§æè¿°ï¼‰
        lines = content.split('\n')
        cleaned_lines = []
        skip_next = False
        
        for i, line in enumerate(lines):
            # è·³è¿‡è¿‡äºè¯¦ç»†çš„åˆ—è¡¨é¡¹
            if 'ç†å­¦é™¢æ•°å­¦ç³»ã€ç‰©ç†ç³»ã€åŒ–å­¦ç³»ã€ç”Ÿç‰©ç³»åˆ†åˆ«å¹¶å…¥' in line:
                # ä¿ç•™æ¦‚æ‹¬ï¼Œè·³è¿‡è¯¦ç»†åˆ—è¡¨
                cleaned_lines.append("ç†å­¦é™¢å„ç³»åˆ†åˆ«è°ƒæ•´è‡³å¤æ—¦å¤§å­¦ã€ä¸Šæµ·ç¬¬ä¸€åŒ»å­¦é™¢ã€åä¸œå¸ˆèŒƒå¤§å­¦ã€å—äº¬å¤§å­¦ç­‰ç›¸å…³é™¢æ ¡ã€‚")
                skip_next = True
            elif skip_next and (line.startswith('ã€€ã€€') or not line.strip()):
                continue
            elif 'æµ™æ±Ÿå¤§å­¦é™¢ç³»è°ƒæ•´çŠ¶å†µå¦‚ä¸‹' in line:
                skip_next = True
                continue
            elif skip_next and not line.startswith('ã€€ã€€'):
                skip_next = False
                if line.strip():
                    cleaned_lines.append(line)
            else:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def structure_content(self, content: str, filename: str) -> Dict:
        """å°†å†…å®¹ç»“æ„åŒ–"""
        # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
        raw_paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        structured_paragraphs = []
        all_time_periods = []
        all_figures = []
        all_locations = []
        
        for para in raw_paragraphs:
            if len(para) < 10:  # è·³è¿‡è¿‡çŸ­çš„æ®µè½
                continue
                
            # æå–æ—¶é—´ä¿¡æ¯
            time_periods = self.extract_time_periods(para)
            all_time_periods.extend(time_periods)
            
            # æå–äººç‰©
            figures = self.extract_figures(para)
            all_figures.extend(figures)
            
            # æå–åœ°ç‚¹
            locations = self.extract_locations(para)
            all_locations.extend(locations)
            
            # ä¸ºæ®µè½æ·»åŠ ç»“æ„ä¿¡æ¯
            structured_para = {
                "content": para,
                "length": len(para),
                "time_periods": time_periods,
                "figures": figures,
                "locations": locations,
                "has_timeline": bool(time_periods)
            }
            structured_paragraphs.append(structured_para)
        
        # æ„å»ºæœ€ç»ˆå†…å®¹ï¼ˆåˆå¹¶æ‰€æœ‰æ®µè½ï¼‰
        final_content = '\n\n'.join([p["content"] for p in structured_paragraphs])
        
        return {
            "content": final_content,
            "paragraphs": structured_paragraphs,
            "time_periods": list(set(all_time_periods)),
            "key_figures": list(set(all_figures)),
            "key_locations": list(set(all_locations))
        }
    
    def extract_time_periods(self, text: str) -> List[str]:
        """æå–æ—¶é—´ä¿¡æ¯"""
        patterns = [
            r'\d{4}å¹´',                    # 1949å¹´
            r'\d{4}è‡³\d{4}å¹´',             # 1937è‡³1945å¹´
            r'\d{4}-\d{4}',                # 1897-1928
            r'ï¼ˆ\d{4}è‡³\d{4}ï¼‰',           # ï¼ˆ1897è‡³1928ï¼‰
        ]
        
        time_periods = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            time_periods.extend(matches)
        
        return time_periods
    
    def extract_figures(self, text: str) -> List[str]:
        """æå–äººç‰©"""
        figures = [
            "ç«ºå¯æ¡¢", "æ—å¯", "è’‹æ¢¦éºŸ", "é™ˆå»ºåŠŸ", "è‹æ­¥é’", "æŸæ˜ŸåŒ—", 
            "è´æ—¶ç’‹", "è”¡é‚¦å", "é©¬ä¸€æµ®", "ä¸°å­æº", "é’±ç©†", "ç‹æ·¦æ˜Œ",
            "è°ˆå®¶æ¡¢", "ææ”¿é“", "ç¨‹å¼€ç”²", "è°·è¶…è±ª", "å¶ç¬ƒæ­£", "æçº¦ç‘Ÿ",
            "é‚µé£˜è", "ä½•ç‡æ—¶", "è’‹æ–¹éœ‡", "è´¹å·©", "äºå­ä¸‰", "é©¬å¯…åˆ",
            "åˆ˜ä¸¹", "è·¯ç”¬ç¥¥", "å¼ å…¶æ˜€", "éƒ‘æ™“æ²§", "é‚µè£´å­", "éƒ­ä»»è¿œ"
        ]
        
        return [f for f in figures if f in text]
    
    def extract_locations(self, text: str) -> List[str]:
        """æå–åœ°ç‚¹"""
        locations = [
            "æ­å·", "å»ºå¾·", "å‰å®‰", "æ³°å’Œ", "å®œå±±", "éµä¹‰", "æ¹„æ½­",
            "å¤©ç›®å±±", "ç¦…æºå¯º", "ç´«é‡‘æ¸¯", "ç‰æ³‰", "ä¹‹æ±Ÿ", "åå®¶æ± ",
            "é¾™æ³‰", "æ¾æœ¨åœº", "æ¹–æ»¨", "èˆŸå±±", "æµ·å®", "å®æ³¢", "ä¸Šæµ·",
            "å—äº¬", "åŒ—äº¬", "è´µå·", "æ±Ÿè¥¿", "å¹¿è¥¿", "æµ™æ±Ÿ"
        ]
        
        return [l for l in locations if l in text]
    
    def save_cleaned_documents(self):
        """ä¿å­˜æ¸…æ´—åçš„æ–‡æ¡£"""
        # ä¿å­˜æ¸…æ´—åçš„å®Œæ•´æ–‡æ¡£
        for doc in self.cleaned_documents:
            filename = f"cleaned_{doc['filename']}"
            filepath = f"raw_data/documents/{filename}"
            
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(doc['content'])
        
        # ä¿å­˜æ¸…æ´—å…ƒæ•°æ®
        metadata = {
            "cleaned_documents": self.cleaned_documents,
            "total_documents": len(self.cleaned_documents),
            "total_paragraphs": sum(len(doc.get('paragraphs', [])) for doc in self.cleaned_documents),
            "total_figures": len(set(f for doc in self.cleaned_documents for f in doc.get('key_figures', []))),
            "total_locations": len(set(l for doc in self.cleaned_documents for l in doc.get('key_locations', []))),
            "cleaning_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open("processed_data/cleaning_metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ¸…æ´—å®Œæˆï¼ç”Ÿæˆæ–‡ä»¶:")
        for doc in self.cleaned_documents:
            print(f"   - cleaned_{doc['filename']}")
        print("   - processed_data/cleaning_metadata.json")

class OptimizedChunker:
    """ä¼˜åŒ–åçš„åˆ†å—å™¨ï¼Œä¸“é—¨é’ˆå¯¹æ¸…æ´—åçš„æ–‡æ¡£"""
    
    def __init__(self, max_chunk_size=350, overlap=30):
        self.max_chunk_size = max_chunk_size
        self.overlap = overlap
    
    def chunk_cleaned_documents(self, cleaned_documents: List[Dict]) -> List[Dict]:
        """å¯¹æ¸…æ´—åçš„æ–‡æ¡£è¿›è¡Œæ™ºèƒ½åˆ†å—"""
        all_chunks = []
        
        for doc in cleaned_documents:
            print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc['filename']}")
            doc_chunks = self.chunk_single_document(doc)
            all_chunks.extend(doc_chunks)
        
        print(f"âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_chunks)} ä¸ªä¼˜åŒ–æ–‡æœ¬å—")
        return all_chunks
    
    def chunk_single_document(self, document: Dict) -> List[Dict]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        chunks = []
        paragraphs = document.get('paragraphs', [])
        
        for i, para in enumerate(paragraphs):
            content = para['content']
            
            # å¦‚æœæ®µè½é•¿åº¦åˆé€‚ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—
            if len(content) <= self.max_chunk_size:
                chunk = self.create_chunk(document, content, i)
                chunks.append(chunk)
            else:
                # éœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                sub_chunks = self.split_paragraph(content, document, i)
                chunks.extend(sub_chunks)
        
        return chunks
    
    def split_paragraph(self, paragraph: str, document: Dict, para_index: int) -> List[Dict]:
        """åˆ†å‰²é•¿æ®µè½"""
        # æŒ‰å¥å­åˆ†å‰²ï¼Œä¿ç•™å¥å­å®Œæ•´æ€§
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', paragraph)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_with_punct = sentence + 'ã€‚'
            
            if current_length + len(sentence_with_punct) > self.max_chunk_size and current_chunk:
                # ä¿å­˜å½“å‰å—
                chunk_content = ''.join(current_chunk)
                chunk = self.create_chunk(document, chunk_content, para_index, len(chunks))
                chunks.append(chunk)
                
                # ä¿ç•™é‡å éƒ¨åˆ†ä»¥ä¿æŒä¸Šä¸‹æ–‡
                overlap_sentences = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk[-1:]
                current_chunk = overlap_sentences + [sentence_with_punct]
                current_length = sum(len(s) for s in current_chunk)
            else:
                current_chunk.append(sentence_with_punct)
                current_length += len(sentence_with_punct)
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunk_content = ''.join(current_chunk)
            chunk = self.create_chunk(document, chunk_content, para_index, len(chunks))
            chunks.append(chunk)
        
        return chunks
    
    def create_chunk(self, document: Dict, content: str, para_index: int, sub_index: int = None) -> Dict:
        """åˆ›å»ºä¼˜åŒ–åçš„æ•°æ®å—"""
        chunk_id = f"{document['filename'].replace('.txt', '')}_p{para_index}"
        if sub_index is not None:
            chunk_id += f"_s{sub_index}"
        
        # ä¸ºæ¯ä¸ªå—æå–ç‹¬ç«‹çš„å…ƒæ•°æ®
        time_periods = self.extract_time_periods(content)
        figures = self.extract_figures(content)
        locations = self.extract_locations(content)
        
        return {
            "id": chunk_id,
            "content": content,
            "source": document['source'],
            "filename": document['filename'],
            "word_count": len(content),
            "time_periods": time_periods,
            "figures": figures,
            "locations": locations,
            "chunk_type": "optimized_chunk",
            "quality_score": self.assess_chunk_quality(content, time_periods, figures),
            "chunk_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def extract_time_periods(self, content: str) -> List[str]:
        """æå–æ—¶é—´ä¿¡æ¯"""
        return re.findall(r'\d{4}å¹´|\d{4}è‡³\d{4}å¹´', content)
    
    def extract_figures(self, content: str) -> List[str]:
        """æå–äººç‰©"""
        figures = ["ç«ºå¯æ¡¢", "æ—å¯", "è’‹æ¢¦éºŸ", "é™ˆå»ºåŠŸ", "è‹æ­¥é’", "æŸæ˜ŸåŒ—", 
                  "è´æ—¶ç’‹", "è”¡é‚¦å", "é©¬ä¸€æµ®", "ä¸°å­æº", "é’±ç©†", "ç‹æ·¦æ˜Œ"]
        return [f for f in figures if f in content]
    
    def extract_locations(self, content: str) -> List[str]:
        """æå–åœ°ç‚¹"""
        locations = ["æ­å·", "å»ºå¾·", "å‰å®‰", "æ³°å’Œ", "å®œå±±", "éµä¹‰", "æ¹„æ½­"]
        return [l for l in locations if l in content]
    
    def assess_chunk_quality(self, content: str, time_periods: List, figures: List) -> float:
        """è¯„ä¼°å—è´¨é‡"""
        score = 0.0
        
        # é•¿åº¦é€‚ä¸­å¾—åˆ†
        if 100 <= len(content) <= 400:
            score += 0.3
        elif len(content) > 400:
            score += 0.1
        
        # æœ‰æ—¶é—´ä¿¡æ¯å¾—åˆ†
        if time_periods:
            score += 0.3
        
        # æœ‰äººç‰©ä¿¡æ¯å¾—åˆ†
        if figures:
            score += 0.2
        
        # æœ‰åœ°ç‚¹ä¿¡æ¯å¾—åˆ†
        if self.extract_locations(content):
            score += 0.2
        
        return min(score, 1.0)

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ–‡æ¡£æ¸…æ´—å’Œä¼˜åŒ–æµç¨‹"""
    print("ğŸš€ å¼€å§‹æµ™å¤§æ ¡å²æ–‡æ¡£æ·±åº¦æ¸…æ´—ä¸ä¼˜åŒ–...")
    
    # 1. æ–‡æ¡£æ¸…æ´—
    print("\nğŸ§¹ é˜¶æ®µ1: æ–‡æ¡£æ·±åº¦æ¸…æ´—")
    cleaner = ZJUDocumentCleaner()
    cleaned_docs = cleaner.clean_all_documents()
    
    if not cleaned_docs:
        print("âŒ æ–‡æ¡£æ¸…æ´—å¤±è´¥")
        return
    
    # 2. ä¼˜åŒ–åˆ†å—
    print("\nâœ‚ï¸ é˜¶æ®µ2: ä¼˜åŒ–åˆ†å—")
    chunker = OptimizedChunker()
    optimized_chunks = chunker.chunk_cleaned_documents(cleaned_docs)
    
    # 3. ä¿å­˜ä¼˜åŒ–åçš„æ•°æ®
    print("\nğŸ’¾ é˜¶æ®µ3: ä¿å­˜æ•°æ®")
    os.makedirs("processed_data", exist_ok=True)
    
    # ä¿å­˜ä¼˜åŒ–åˆ†å—
    with open("processed_data/optimized_chunks.json", "w", encoding="utf-8") as f:
        json.dump(optimized_chunks, f, ensure_ascii=False, indent=2)
    
    # 4. ç»Ÿè®¡ä¿¡æ¯
    total_words = sum(chunk['word_count'] for chunk in optimized_chunks)
    avg_chunk_size = total_words / len(optimized_chunks) if optimized_chunks else 0
    high_quality_chunks = sum(1 for c in optimized_chunks if c.get('quality_score', 0) > 0.7)
    
    print(f"""
ğŸ‰ æ–‡æ¡£æ¸…æ´—ä¸ä¼˜åŒ–å®Œæˆï¼

ğŸ“Š ä¼˜åŒ–ç»“æœç»Ÿè®¡:
â”œâ”€â”€ æ¸…æ´—æ–‡æ¡£: {len(cleaned_docs)} ä¸ª
â”œâ”€â”€ ä¼˜åŒ–æ–‡æœ¬å—: {len(optimized_chunks)} ä¸ª
â”œâ”€â”€ æ€»å­—æ•°: {total_words} å­—
â”œâ”€â”€ å¹³å‡å—å¤§å°: {avg_chunk_size:.1f} å­—
â”œâ”€â”€ é«˜è´¨é‡å—: {high_quality_chunks} ä¸ª (è´¨é‡åˆ†>0.7)
â””â”€â”€ å¹³å‡è´¨é‡åˆ†: {sum(c.get('quality_score', 0) for c in optimized_chunks) / len(optimized_chunks):.2f}

ğŸ’¾ ç”Ÿæˆæ–‡ä»¶:
â”œâ”€â”€ raw_data/documents/cleaned_*.txt (æ¸…æ´—åçš„æ–‡æ¡£)
â”œâ”€â”€ processed_data/cleaning_metadata.json (æ¸…æ´—å…ƒæ•°æ®)
â””â”€â”€ processed_data/optimized_chunks.json (ä¼˜åŒ–åˆ†å—æ•°æ®)

ğŸ¯ æ¥ä¸‹æ¥è¿è¡Œ: python rebuild_vector_db.py
    """)

if __name__ == "__main__":
    main()